"""
Module for working with S3-compatible storage (e.g., MinIO).
"""

import io
from contextlib import asynccontextmanager
from typing import BinaryIO, Optional

import aioboto3
from botocore.exceptions import ClientError

from app.core.config import settings
from app.core.logger import logger


class S3Storage:
    """Class for interacting with an S3-compatible storage backend."""

    def __init__(self):
        self.endpoint_url = settings.S3_ENDPOINT_URL
        self.public_url = settings.S3_PUBLIC_URL
        self.access_key_id = settings.S3_ACCESS_KEY_ID
        self.secret_access_key = settings.S3_SECRET_ACCESS_KEY
        self.region = settings.S3_REGION
        self.bucket_name = settings.S3_BUCKET_NAME
        self.use_ssl = settings.S3_USE_SSL

        self.session = aioboto3.Session()

    @asynccontextmanager
    async def get_client(self):
        """Return an S3 client."""
        async with self.session.client(
            "s3",
            endpoint_url=self.endpoint_url,
            aws_access_key_id=self.access_key_id,
            aws_secret_access_key=self.secret_access_key,
            region_name=self.region,
            use_ssl=self.use_ssl,
        ) as client:
            yield client

    async def ensure_bucket_exists(self):
        """Create the bucket if it does not exist."""
        try:
            async with self.get_client() as client:
                await client.head_bucket(Bucket=self.bucket_name)
                logger.debug(f"Bucket {self.bucket_name} already exists")
        except ClientError as e:
            error_code = e.response.get("Error", {}).get("Code", "")
            if error_code == "404":
                async with self.get_client() as client:
                    await client.create_bucket(Bucket=self.bucket_name)
                    logger.info(f"Created bucket {self.bucket_name}")
            else:
                logger.error(f"Error checking bucket: {e}")
                raise

    async def upload_file(
        self,
        file_obj: BinaryIO,
        object_key: str,
        content_type: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> str:
        """
        Upload a file object into S3.

        Args:
            file_obj: File-like object
            object_key: S3 object key (path)
            content_type: MIME type
            metadata: Additional metadata

        Returns:
            Absolute URL of the uploaded file
        """
        await self.ensure_bucket_exists()

        extra_args = {}
        if content_type:
            extra_args["ContentType"] = content_type
        if metadata:
            extra_args["Metadata"] = metadata

        async with self.get_client() as client:
            await client.upload_fileobj(
                file_obj, self.bucket_name, object_key, ExtraArgs=extra_args
            )

        logger.info(f"Uploaded file to s3://{self.bucket_name}/{object_key}")
        return f"{self.endpoint_url}/{self.bucket_name}/{object_key}"

    async def upload_bytes(
        self,
        data: bytes,
        object_key: str,
        content_type: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> str:
        """
        Upload raw bytes into S3.

        Args:
            data: Bytes to upload
            object_key: S3 object key
            content_type: MIME type
            metadata: Additional metadata
        """
        file_obj = io.BytesIO(data)
        return await self.upload_file(file_obj, object_key, content_type, metadata)

    async def download_file(self, object_key: str) -> bytes:
        """
        Download an object from S3 as bytes.

        Args:
            object_key: S3 object key
        """
        async with self.get_client() as client:
            response = await client.get_object(Bucket=self.bucket_name, Key=object_key)
            return await response["Body"].read()

    async def download_file_stream(self, object_key: str):
        """
        Stream an object from S3 in chunks.

        Args:
            object_key: S3 object key

        Yields:
            Chunks of file data
        """
        async with self.get_client() as client:
            response = await client.get_object(Bucket=self.bucket_name, Key=object_key)
            async for chunk in response["Body"]:
                yield chunk

    async def delete_file(self, object_key: str) -> bool:
        """
        Delete an object from S3.

        Args:
            object_key: S3 object key

        Returns:
            True if deleted, False if not found
        """
        try:
            async with self.get_client() as client:
                await client.delete_object(Bucket=self.bucket_name, Key=object_key)
                logger.info(f"Deleted file s3://{self.bucket_name}/{object_key}")
                return True
        except ClientError as e:
            logger.error(f"Error deleting file: {e}")
            return False

    async def file_exists(self, object_key: str) -> bool:
        """Check if an object exists in S3."""
        try:
            async with self.get_client() as client:
                await client.head_object(Bucket=self.bucket_name, Key=object_key)
                return True
        except ClientError as e:
            if e.response.get("Error", {}).get("Code") == "404":
                return False
            raise

    async def get_file_url(self, object_key: str, expires_in: int = 3600) -> str:
        """
        Generate a temporary presigned URL for a file.

        Args:
            object_key: S3 object key
            expires_in: Expiration in seconds
        """
        async with self.get_client() as client:
            presigned_url = await client.generate_presigned_url(
                "get_object",
                Params={"Bucket": self.bucket_name, "Key": object_key},
                ExpiresIn=expires_in,
            )
            if self.endpoint_url != self.public_url:
                presigned_url = presigned_url.replace(
                    self.endpoint_url, self.public_url
                )
            return presigned_url

    async def list_files(self, prefix: str = "") -> list[dict]:
        """
        List files in the bucket optionally filtered by prefix.

        Returns:
            List of dicts containing object metadata
        """
        files = []
        async with self.get_client() as client:
            paginator = client.get_paginator("list_objects_v2")
            async for page in paginator.paginate(
                Bucket=self.bucket_name, Prefix=prefix
            ):
                for obj in page.get("Contents", []):
                    files.append(
                        {
                            "key": obj["Key"],
                            "size": obj["Size"],
                            "last_modified": obj["LastModified"].isoformat(),
                        }
                    )
        return files
_storage_instance: Optional[S3Storage] = None


async def get_s3_client() -> S3Storage:
    """FastAPI dependency that returns a singleton S3Storage instance."""
    global _storage_instance
    if _storage_instance is None:
        _storage_instance = S3Storage()
        await _storage_instance.ensure_bucket_exists()
    return _storage_instance
